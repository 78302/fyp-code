{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8532d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import torchvision\n",
    "\n",
    "INPUT_SIZE = 40\n",
    "HIDDEN_SIZE = 512  # units inside the lstm\n",
    "# DROP_RATE = 0.2  # drop-out rate\n",
    "LAYERS = 1  # number of lstm layers, will be increased to 4\n",
    "\n",
    "\n",
    "class toy_lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_lstm, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=LAYERS,\n",
    "#             dropout=DROP_RATE,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 40)  # fully connected layer\n",
    "        self.h_s = None\n",
    "        self.h_c = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_s, h_c) = self.rnn(x)\n",
    "        output = self.fc(r_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a25b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:9\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "9\n",
      "(652, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:26439\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "26439\n",
      "(693, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:54509\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "54509\n",
      "(1069, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:97619\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "97619\n",
      "(449, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:115929\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "115929\n",
      "(373, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:131199\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "131199\n",
      "(550, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:153549\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "153549\n",
      "(344, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:167659\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "167659\n",
      "(636, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:193449\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "193449\n",
      "(757, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:224079\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "224079\n",
      "(763, 40)\n",
      "/home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.1.ark:254949\\r\\n'\n",
      "/raw_fbank_train_si284.1.ark\n",
      "254949\n",
      "(924, 40)\n"
     ]
    }
   ],
   "source": [
    "import kaldiark\n",
    "\n",
    "# Read data index from the total scp file\n",
    "\n",
    "count = 0\n",
    "\n",
    "with open('./data/raw_fbank_train_si284.1.scp', 'rb') as scp_file:  # use '../remote/data/wsj/fbank/' replace '/data/'\n",
    "    lines = scp_file.readlines()\n",
    "    for line in lines: # line is like b'4avc040p /home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.10.ark:2769059\\n'\n",
    "        temp = str(line).split()[1]\n",
    "        print(temp)\n",
    "        file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "        pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "        print(file_loc)\n",
    "        print(pointer)\n",
    "\n",
    "        # According to the file name and pointer to get the matrix\n",
    "        with open('./data' + file_loc, 'rb') as ark_file:  # use '../remote/data' + file_loc replace './data/' + file_loc\n",
    "            ark_file.seek(int(pointer))\n",
    "            utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            print(utt_mat.shape)  \n",
    "        \n",
    "        count = count + 1\n",
    "        if count > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6242663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete!\n",
      "dev complete!\n",
      "iter: [1/20], train_loss: 65.319239, valid_loss: 40.543747, best_valid_loss: 40.543747, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [2/20], train_loss: 16.876287, valid_loss: 21.307021, best_valid_loss: 21.307021, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [3/20], train_loss: 12.337894, valid_loss: 17.886119, best_valid_loss: 17.886119, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [4/20], train_loss: 11.849856, valid_loss: 17.893142, best_valid_loss: 17.886119, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [5/20], train_loss: 11.805400, valid_loss: 17.802566, best_valid_loss: 17.802566, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [6/20], train_loss: 11.673864, valid_loss: 17.814047, best_valid_loss: 17.802566, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [7/20], train_loss: 11.637706, valid_loss: 17.654215, best_valid_loss: 17.654215, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [8/20], train_loss: 11.720034, valid_loss: 17.824579, best_valid_loss: 17.654215, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [9/20], train_loss: 11.831823, valid_loss: 18.152105, best_valid_loss: 17.654215, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [10/20], train_loss: 12.003723, valid_loss: 18.687731, best_valid_loss: 17.654215, lr: 0.1000000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [11/20], train_loss: 11.878866, valid_loss: 18.688328, best_valid_loss: 17.654215, lr: 0.0100000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [12/20], train_loss: 11.870848, valid_loss: 18.697507, best_valid_loss: 17.654215, lr: 0.0100000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [13/20], train_loss: 11.884196, valid_loss: 18.680401, best_valid_loss: 17.654215, lr: 0.0100000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [14/20], train_loss: 11.879790, valid_loss: 18.787499, best_valid_loss: 17.654215, lr: 0.0100000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [15/20], train_loss: 11.889289, valid_loss: 18.993024, best_valid_loss: 17.654215, lr: 0.0100000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [16/20], train_loss: 11.611152, valid_loss: 17.656562, best_valid_loss: 17.654215, lr: 0.0010000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [17/20], train_loss: 11.394530, valid_loss: 17.022310, best_valid_loss: 17.022310, lr: 0.0010000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [18/20], train_loss: 11.373350, valid_loss: 17.324919, best_valid_loss: 17.022310, lr: 0.0010000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [19/20], train_loss: 11.374683, valid_loss: 17.393555, best_valid_loss: 17.022310, lr: 0.0010000\n",
      "train complete!\n",
      "dev complete!\n",
      "iter: [20/20], train_loss: 11.376508, valid_loss: 17.310338, best_valid_loss: 17.022310, lr: 0.0010000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import kaldiark\n",
    "from apc import toy_lstm\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # \n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCH = 20\n",
    "\n",
    "rnn = toy_lstm().to(device)  \n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)  # optimize all parameters\n",
    "loss_func = nn.MSELoss()\n",
    "# Learning rate decay schedule\n",
    "mult_step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                           milestones=[EPOCH // 2, EPOCH // 4 * 3], gamma=0.1)\n",
    "\n",
    "# Predefine the prediction gap\n",
    "K = 8  # predefine the gap\n",
    "\n",
    "\n",
    "# Train + Dev\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "min_valid_loss = np.inf\n",
    "for i in range(EPOCH):\n",
    "    total_train_loss = []\n",
    "    rnn.train()  # Training\n",
    "    \n",
    "    # Use the total scp files\n",
    "    # Read data index from the total scp file\n",
    "    with open('./data/raw_fbank_train_si284.1.scp', 'rb') as scp_file:\n",
    "        lines = scp_file.readlines()\n",
    "        for line in lines[:15]:\n",
    "            temp = str(line).split()[1]\n",
    "            file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "            pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "#             print(file_loc, pointer)\n",
    "\n",
    "            # According to the file name and pointer to get the matrix\n",
    "            with open('./data' + file_loc, 'rb') as ark_file:\n",
    "                ark_file.seek(int(pointer))\n",
    "                utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            \n",
    "                utt_mat = np.expand_dims(utt_mat, axis=0)  # expand a new dimension as batch\n",
    "                utt_mat = torch.Tensor(utt_mat).to(device)   # change data to tensor\n",
    "\n",
    "                output = rnn(utt_mat[:, :-K, :])\n",
    "                \n",
    "#                 print(utt_mat.shape, output.shape)\n",
    "\n",
    "                loss = loss_func(output, utt_mat[:, K:, :])  # compute the difference\n",
    "                optimizer.zero_grad()  # clear gradients for this training step\n",
    "                loss.backward()  # back-prop\n",
    "                optimizer.step()  # gradients\n",
    "                total_train_loss.append(loss.item())\n",
    "        train_loss.append(np.mean(total_train_loss))\n",
    "    print('train complete!')\n",
    "\n",
    "    total_valid_loss = []\n",
    "    rnn.eval()  # Validation\n",
    "    \n",
    "    # Use one of scp files\n",
    "    # Read data index from the total scp file\n",
    "    with open('./data/raw_fbank_train_si284.2.scp', 'rb') as scp_file:  # change 1 to dev \n",
    "        lines = scp_file.readlines()\n",
    "        for line in lines[:3]:\n",
    "            temp = str(line).split()[1]\n",
    "            file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "            pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "\n",
    "            # According to the file name and pointer to get the matrix\n",
    "            with open('./data' + file_loc, 'rb') as ark_file:\n",
    "                ark_file.seek(int(pointer))\n",
    "                utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            \n",
    "                utt_mat = np.expand_dims(utt_mat, axis=0)  # expand a new dimension as batch\n",
    "                utt_mat = torch.Tensor(utt_mat).to(device)   # change data to tensor\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = rnn(utt_mat[:, :-K, :])  # rnn output\n",
    "                    \n",
    "#                     print(utt_mat_mat.shape, output.shape)\n",
    "\n",
    "                loss = loss_func(output, utt_mat[:, K:, :])\n",
    "            total_valid_loss.append(loss.item())\n",
    "        valid_loss.append(np.mean(total_valid_loss))\n",
    "    print('dev complete!')\n",
    "\n",
    "    if (valid_loss[-1] < min_valid_loss):\n",
    "        torch.save({'epoch': i, 'model': rnn, 'train_loss': train_loss,\n",
    "                    'valid_loss': valid_loss}, './LSTM.model')\n",
    "        min_valid_loss = valid_loss[-1]\n",
    "\n",
    "    # Log\n",
    "    log_string = ('iter: [{:d}/{:d}], train_loss: {:0.6f}, valid_loss: {:0.6f}, '\n",
    "                  'best_valid_loss: {:0.6f}, lr: {:0.7f}').format((i + 1), EPOCH,\n",
    "                                                                  train_loss[-1],\n",
    "                                                                  valid_loss[-1],\n",
    "                                                                  min_valid_loss,\n",
    "                                                                  optimizer.param_groups[0]['lr'])\n",
    "    mult_step_scheduler.step()  # 学习率更新\n",
    "    print(log_string)  # 打印日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a96264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
