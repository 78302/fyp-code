{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf06a8e",
   "metadata": {},
   "source": [
    "# APC Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import torchvision\n",
    "\n",
    "INPUT_SIZE = 40\n",
    "HIDDEN_SIZE = 512  # units inside the lstm\n",
    "# DROP_RATE = 0.2  # drop-out rate\n",
    "LAYERS = 1  # number of lstm layers, will be increased to 4\n",
    "\n",
    "\n",
    "class toy_lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_lstm, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=LAYERS,\n",
    "#             dropout=DROP_RATE,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 40)  # fully connected layer\n",
    "        self.h_s = None\n",
    "        self.h_c = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_s, h_c) = self.rnn(x)\n",
    "        output = self.fc(r_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a25b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaldiark\n",
    "\n",
    "# # Read data index from the total scp file\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# with open('./data/raw_fbank_train_si284.1.scp', 'rb') as scp_file:  # use '../remote/data/wsj/fbank/' replace '/data/'\n",
    "#     lines = scp_file.readlines()\n",
    "#     for line in lines: # line is like b'4avc040p /home/htang2/kaldi/wsj/fbank/raw_fbank_train_si284.10.ark:2769059\\n'\n",
    "#         temp = str(line).split()[1]\n",
    "#         print(temp)\n",
    "#         file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "#         pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "#         print(file_loc)\n",
    "#         print(pointer)\n",
    "\n",
    "#         # According to the file name and pointer to get the matrix\n",
    "#         with open('./data' + file_loc, 'rb') as ark_file:  # use '../remote/data' + file_loc replace './data/' + file_loc\n",
    "#             ark_file.seek(int(pointer))\n",
    "#             utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "#             print(utt_mat.shape)  \n",
    "        \n",
    "#         count = count + 1\n",
    "#         if count > 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6242663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: [1/60], train_loss: 185.335281, valid_loss: 107.390554, best_valid_loss: 107.390554, lr: 0.0050000\n",
      "iter: [2/60], train_loss: 118.975380, valid_loss: 66.732348, best_valid_loss: 66.732348, lr: 0.0050000\n",
      "iter: [3/60], train_loss: 74.532879, valid_loss: 37.939902, best_valid_loss: 37.939902, lr: 0.0050000\n",
      "iter: [4/60], train_loss: 42.338984, valid_loss: 21.579492, best_valid_loss: 21.579492, lr: 0.0050000\n",
      "iter: [5/60], train_loss: 22.557779, valid_loss: 16.344495, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [6/60], train_loss: 13.814932, valid_loss: 19.274874, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [7/60], train_loss: 13.090472, valid_loss: 26.252875, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [8/60], train_loss: 16.465832, valid_loss: 33.299493, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [9/60], train_loss: 20.382216, valid_loss: 37.702118, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [10/60], train_loss: 22.570373, valid_loss: 38.490917, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [11/60], train_loss: 22.241153, valid_loss: 35.775551, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [12/60], train_loss: 19.916075, valid_loss: 31.479891, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [13/60], train_loss: 17.029061, valid_loss: 26.915883, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [14/60], train_loss: 14.421190, valid_loss: 22.884276, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [15/60], train_loss: 12.644375, valid_loss: 19.857562, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [16/60], train_loss: 11.865329, valid_loss: 17.857606, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [17/60], train_loss: 11.926786, valid_loss: 16.824047, best_valid_loss: 16.344495, lr: 0.0050000\n",
      "iter: [18/60], train_loss: 12.327363, valid_loss: 16.303992, best_valid_loss: 16.303992, lr: 0.0050000\n",
      "iter: [19/60], train_loss: 12.682915, valid_loss: 16.042475, best_valid_loss: 16.042475, lr: 0.0050000\n",
      "iter: [20/60], train_loss: 12.778398, valid_loss: 15.939068, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [21/60], train_loss: 12.592671, valid_loss: 16.005062, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [22/60], train_loss: 12.240403, valid_loss: 16.286565, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [23/60], train_loss: 11.877368, valid_loss: 16.791387, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [24/60], train_loss: 11.618578, valid_loss: 17.454460, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [25/60], train_loss: 11.500014, valid_loss: 18.150913, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [26/60], train_loss: 11.487948, valid_loss: 18.741211, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [27/60], train_loss: 11.517526, valid_loss: 19.120375, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [28/60], train_loss: 11.533529, valid_loss: 19.247209, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [29/60], train_loss: 11.512887, valid_loss: 19.144683, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [30/60], train_loss: 11.463619, valid_loss: 18.878307, best_valid_loss: 15.939068, lr: 0.0050000\n",
      "iter: [31/60], train_loss: 11.426764, valid_loss: 18.840885, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [32/60], train_loss: 11.419742, valid_loss: 18.794208, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [33/60], train_loss: 11.411001, valid_loss: 18.740578, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [34/60], train_loss: 11.401130, valid_loss: 18.682035, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [35/60], train_loss: 11.390662, valid_loss: 18.620356, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [36/60], train_loss: 11.380048, valid_loss: 18.557067, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [37/60], train_loss: 11.369654, valid_loss: 18.493443, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [38/60], train_loss: 11.359757, valid_loss: 18.430542, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [39/60], train_loss: 11.350563, valid_loss: 18.369211, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [40/60], train_loss: 11.342200, valid_loss: 18.310118, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [41/60], train_loss: 11.334740, valid_loss: 18.253774, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [42/60], train_loss: 11.328201, valid_loss: 18.200551, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [43/60], train_loss: 11.322568, valid_loss: 18.150693, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [44/60], train_loss: 11.317795, valid_loss: 18.104359, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [45/60], train_loss: 11.313814, valid_loss: 18.061617, best_valid_loss: 15.939068, lr: 0.0005000\n",
      "iter: [46/60], train_loss: 11.311796, valid_loss: 18.057674, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [47/60], train_loss: 11.311505, valid_loss: 18.054000, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [48/60], train_loss: 11.311231, valid_loss: 18.050543, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [49/60], train_loss: 11.310972, valid_loss: 18.047269, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [50/60], train_loss: 11.310726, valid_loss: 18.044142, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [51/60], train_loss: 11.310491, valid_loss: 18.041143, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [52/60], train_loss: 11.310265, valid_loss: 18.038245, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [53/60], train_loss: 11.310047, valid_loss: 18.035432, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [54/60], train_loss: 11.309836, valid_loss: 18.032693, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [55/60], train_loss: 11.309631, valid_loss: 18.030011, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [56/60], train_loss: 11.309430, valid_loss: 18.027380, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [57/60], train_loss: 11.309235, valid_loss: 18.024790, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [58/60], train_loss: 11.309043, valid_loss: 18.022238, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [59/60], train_loss: 11.308856, valid_loss: 18.019716, best_valid_loss: 15.939068, lr: 0.0000500\n",
      "iter: [60/60], train_loss: 11.308671, valid_loss: 18.017221, best_valid_loss: 15.939068, lr: 0.0000500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import kaldiark\n",
    "from apc import toy_lstm\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # \n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCH = 60\n",
    "\n",
    "rnn = toy_lstm().to(device)  \n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)  # optimize all parameters\n",
    "loss_func = nn.MSELoss()\n",
    "# Learning rate decay schedule\n",
    "mult_step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                           milestones=[EPOCH // 2, EPOCH // 4 * 3], gamma=0.1)\n",
    "\n",
    "# Predefine the prediction gap\n",
    "K = 2  # predefine the gap\n",
    "\n",
    "# Train + Dev\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "min_valid_loss = np.inf\n",
    "for i in range(EPOCH):\n",
    "    total_train_loss = []\n",
    "    rnn.train()  # Training\n",
    "    \n",
    "    # Use the total scp files\n",
    "    # Read data index from the total scp file\n",
    "    with open('./data/raw_fbank_train_si284.1.scp', 'rb') as scp_file:\n",
    "        lines = scp_file.readlines()\n",
    "        for line in lines[:2]:  # use 2 utt to test\n",
    "            temp = str(line).split()[1]\n",
    "            file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "            pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "#             print(file_loc, pointer)\n",
    "\n",
    "            # According to the file name and pointer to get the matrix\n",
    "            with open('./data' + file_loc, 'rb') as ark_file:\n",
    "                ark_file.seek(int(pointer))\n",
    "                utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            \n",
    "                utt_mat = np.expand_dims(utt_mat, axis=0)  # expand a new dimension as batch\n",
    "                utt_mat = torch.Tensor(utt_mat).to(device)   # change data to tensor\n",
    "                output = rnn(utt_mat[:, :-K, :])\n",
    "                \n",
    "#                 print(utt_mat.shape, output.shape)\n",
    "\n",
    "                loss = loss_func(output, utt_mat[:, K:, :])  # compute the difference\n",
    "                optimizer.zero_grad()  # clear gradients for this training step\n",
    "                loss.backward()  # back-prop\n",
    "                optimizer.step()  # gradients\n",
    "                total_train_loss.append(loss.item())\n",
    "        train_loss.append(np.mean(total_train_loss))\n",
    "    # print('train complete!')\n",
    "\n",
    "    total_valid_loss = []\n",
    "    rnn.eval()  # Validation\n",
    "    \n",
    "    # Use one of scp files\n",
    "    # Read data index from the total scp file\n",
    "    with open('./data/raw_fbank_train_si284.2.scp', 'rb') as scp_file:  # change 1 to dev \n",
    "        lines = scp_file.readlines()\n",
    "        for line in lines[:3]:\n",
    "            temp = str(line).split()[1]\n",
    "            file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "            pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "\n",
    "            # According to the file name and pointer to get the matrix\n",
    "            with open('./data' + file_loc, 'rb') as ark_file:\n",
    "                ark_file.seek(int(pointer))\n",
    "                utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            \n",
    "                utt_mat = np.expand_dims(utt_mat, axis=0)  # expand a new dimension as batch\n",
    "                utt_mat = torch.Tensor(utt_mat).to(device)   # change data to tensor\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = rnn(utt_mat[:, :-K, :])  # rnn output\n",
    "                    \n",
    "#                     print(utt_mat_mat.shape, output.shape)\n",
    "\n",
    "                loss = loss_func(output, utt_mat[:, K:, :])\n",
    "            total_valid_loss.append(loss.item())\n",
    "        valid_loss.append(np.mean(total_valid_loss))\n",
    "    # print('dev complete!')\n",
    "    \n",
    "    # save the net\n",
    "\n",
    "    min_valid_loss = np.min(valid_loss)\n",
    "    \n",
    "    if ((i + 1) % 10 == 0):\n",
    "        torch.save({'epoch': i + 1, 'state_dict': rnn.state_dict(), 'train_loss': train_loss,\n",
    "                    'valid_loss': valid_loss, 'optimizer': optimizer.state_dict()},\n",
    "                    './model/Epoch{:d}.pth.tar'.format((i + 1), min_valid_loss))\n",
    "    \n",
    "\n",
    "    # Log\n",
    "    log_string = ('iter: [{:d}/{:d}], train_loss: {:0.6f}, valid_loss: {:0.6f}, '\n",
    "                  'best_valid_loss: {:0.6f}, lr: {:0.7f}').format((i + 1), EPOCH,\n",
    "                                                                  train_loss[-1],\n",
    "                                                                  valid_loss[-1],\n",
    "                                                                  min_valid_loss,\n",
    "                                                                  optimizer.param_groups[0]['lr'])\n",
    "    mult_step_scheduler.step()  # 学习率更新\n",
    "    print(log_string)  # 打印日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8eeb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  \n",
    "matplotlib.use('Agg')\n",
    "y = train_loss\n",
    "x = np.arange(0,len(train_loss))\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "ax.plot(x,y,'r--',label='type1')\n",
    "\n",
    "ax.set_title('Loss',fontsize=18)\n",
    "ax.set_xlabel('epoch', fontsize=18,fontfamily = 'sans-serif',fontstyle='italic')\n",
    "ax.set_ylabel('loss', fontsize='x-large',fontstyle='oblique')\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig(\"loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adb3d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model, optimizer):\n",
    "    \n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer\n",
    "\n",
    "PATH = './model/Epoch60.pth.tar'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 对照-使用默认参数\n",
    "rnn_raw = toy_lstm().to(device)\n",
    "optimizer_raw = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)  # optimize all parameters\n",
    "\n",
    "# 使用预训练参数\n",
    "rnn_pretrain = toy_lstm().to(device)\n",
    "optimizer_pretrain = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)  # optimize all parameters\n",
    "rnn_pretrain, optimizer_pretrain = load_model(PATH, rnn_pretrain, optimizer_pretrain)\n",
    "\n",
    "rnn_pretrain.eval()\n",
    "rnn_raw.eval()\n",
    "\n",
    "# get 2 utt mats:\n",
    "# Predefine the prediction gap\n",
    "K = 2  # predefine the gap\n",
    "\n",
    "ori_mat = []\n",
    "pre_mat = []\n",
    "with open('./data/raw_fbank_train_si284.1.scp', 'rb') as scp_file:\n",
    "    lines = scp_file.readlines()\n",
    "    # for line in lines[:2]:  # use 1 utt to test\n",
    "    temp = str(lines[0]).split()[1]\n",
    "    file_loc = temp.split(':')[0][28:]  # ark file path; keep [18:]\n",
    "    pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "\n",
    "    # According to the file name and pointer to get the matrix\n",
    "    with open('./data' + file_loc, 'rb') as ark_file:\n",
    "        ark_file.seek(int(pointer))\n",
    "        utt_mat = kaldiark.parse_feat_matrix(ark_file)\n",
    "            \n",
    "        utt_mat = np.expand_dims(utt_mat, axis=0)  # expand a new dimension as batch\n",
    "        utt_mat = torch.Tensor(utt_mat).to(device)   # change data to tensor\n",
    "        \n",
    "        output_raw = rnn_raw(utt_mat[:, :-K, :])\n",
    "        output_pretrain = rnn_pretrain(utt_mat[:, :-K, :])\n",
    "                \n",
    "        ori_mat.append(utt_mat[0, :-K, :])\n",
    "        pre_mat.append(output_raw[0])\n",
    "        pre_mat.append(output_pretrain[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcfd1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = ori_mat[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11eb4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = pre_mat[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f03e69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = pre_mat[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "503f6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Save Image Function\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "cax = plt.imshow(m1, cmap='viridis')\n",
    "# # set up colorbar\n",
    "# cbar = plt.colorbar(cax, extend='both', drawedges = False)\n",
    "# cbar.set_label('Intensity',size=36, weight =  'bold')\n",
    "# cbar.ax.tick_params( labelsize=18 )\n",
    "# cbar.minorticks_on()\n",
    "# # set up axis labels\n",
    "# ticks=np.arange(0,m1.shape[0],1)\n",
    "# ## For x ticks\n",
    "# plt.xticks(ticks, fontsize=12, fontweight = 'bold')\n",
    "# ax.set_xticklabels(ticks)\n",
    "# ## For y ticks\n",
    "# plt.yticks(ticks, fontsize=12, fontweight = 'bold')\n",
    "# ax.set_yticklabels(ticks)\n",
    "plt.savefig('origin.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "301b5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Image Function\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "cax = plt.imshow(m2, cmap='viridis')\n",
    "plt.savefig('raw.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f60ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Image Function\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.gca()\n",
    "cax = plt.imshow(m3, cmap='viridis')\n",
    "plt.savefig('pretrained.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d8f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8add9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae262ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84151745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9013,  3.5482,  3.9370,  ..., 12.6050, 12.6487, 12.5332],\n",
      "        [ 3.8091,  3.2036,  3.6625,  ..., 12.8245, 12.6994, 12.1048],\n",
      "        [ 4.3622,  3.4333,  4.2115,  ..., 12.7971, 12.9021, 12.0844],\n",
      "        ...,\n",
      "        [ 4.1779,  3.2036,  3.3879,  ..., 12.6324, 12.7501, 11.7375],\n",
      "        [ 3.8091,  2.1698,  3.7997,  ..., 12.7971, 12.3445, 11.7579],\n",
      "        [ 3.3482,  3.0887,  3.3879,  ..., 13.0715, 13.1809, 12.4312]])\n",
      "tensor([[ 3.9013,  3.5482,  3.9370,  ..., 12.6050, 12.6487, 12.5332],\n",
      "        [ 3.8091,  3.2036,  3.6625,  ..., 12.8245, 12.6994, 12.1048],\n",
      "        [ 4.3622,  3.4333,  4.2115,  ..., 12.7971, 12.9021, 12.0844],\n",
      "        ...,\n",
      "        [ 3.9013,  4.3522,  3.1134,  ..., 12.8245, 12.6233, 12.3904],\n",
      "        [ 3.7170,  4.6968,  4.6233,  ..., 12.9343, 12.9528, 12.4108],\n",
      "        [ 4.1779,  3.2036,  3.3879,  ..., 12.6324, 12.7501, 11.7375]])\n",
      "tensor([[-0.0204, -0.0401,  0.1698,  ...,  0.0237, -0.0143,  0.0480],\n",
      "        [ 0.0033, -0.0277,  0.1911,  ...,  0.0355, -0.0017,  0.0486],\n",
      "        [ 0.0229,  0.0054,  0.2038,  ...,  0.0396,  0.0291,  0.0517],\n",
      "        ...,\n",
      "        [ 0.0650,  0.0404,  0.2205,  ...,  0.0443,  0.1243,  0.0175],\n",
      "        [ 0.0562,  0.0354,  0.2289,  ...,  0.0473,  0.1081,  0.0246],\n",
      "        [ 0.0705,  0.0546,  0.2297,  ...,  0.0468,  0.1078,  0.0308]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([[ 7.1982,  7.3003,  7.7380,  ...,  8.3063,  8.5552,  8.2469],\n",
      "        [ 9.6438,  9.7859, 10.4431,  ..., 11.1874, 11.5854, 11.1365],\n",
      "        [10.0378, 10.1777, 10.8667,  ..., 11.6556, 12.0799, 11.6058],\n",
      "        ...,\n",
      "        [10.1030, 10.2439, 10.9385,  ..., 11.7354, 12.1617, 11.6827],\n",
      "        [10.1031, 10.2435, 10.9391,  ..., 11.7349, 12.1623, 11.6826],\n",
      "        [10.1047, 10.2435, 10.9370,  ..., 11.7358, 12.1616, 11.6829]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(utt_mat[0])\n",
    "print(ori_mat[0])\n",
    "print(pre_mat[0])\n",
    "print(pre_mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb6847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147517be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['rnn.weight_ih_l0', 'rnn.weight_hh_l0', 'rnn.bias_ih_l0', 'rnn.bias_hh_l0', 'fc.weight', 'fc.bias'])\n",
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "load_dict = torch.load('./model/Epoch30.pth.tar')['state_dict']\n",
    "print(load_dict.keys())\n",
    "print(type(load_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502385c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4b3339c",
   "metadata": {},
   "source": [
    "# Probing Task Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a96264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class classification_net(torch.nn.Module):  \n",
    "    def __init__(self, input_size, hidden_size, output_size):  \n",
    "        super(classification_net, self).__init__()  \n",
    "        self.hidden_layer = torch.nn.Linear(input_size, hidden_size)  \n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)  \n",
    "    def forward(self, x):  \n",
    "        x = torch.relu(self.hidden_layer(x))  \n",
    "        x = self.out(x) \n",
    "        x = torch.nn.functional.softmax(x)  \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca39923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_net(\n",
      "  (hidden_layer): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = classification_net(input_size=40, hidden_size=512, output_size=2)  \n",
    "print(net)  \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  \n",
    "loss_func = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22e163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0013b9c",
   "metadata": {},
   "source": [
    "Read bpali data & fbank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f64d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n"
     ]
    }
   ],
   "source": [
    "bpali_file = open('./data/train-si284.bpali', 'rb')\n",
    "fbank_scp = open('./data/si284-0.9-train.fbank.scp', 'rb')\n",
    "with open('./data/si284-0.9-train.bpali.scp', 'rb') as scp_file:\n",
    "    lines = scp_file.readlines()\n",
    "    for line in lines[:1]:\n",
    "        temp = str(line).split()[1]\n",
    "        pointer = temp.split(':')[1][:-3].replace('\\\\r', '')  # pointer to the utterance\n",
    "\n",
    "        # Windows contain \\r\\n instead of \\n\n",
    "        bpali_file.seek(int(pointer)-1)\n",
    "        transcript = str(bpali_file.readlines()[1])[2:-3].replace('\\\\r', '')\n",
    "        out_list = transcript.split()\n",
    "        print(len(out_list))\n",
    "        \n",
    "bpali_file.close()\n",
    "fbank_scp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848b583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
